{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rcbusinesstechlab/realtime-face-recognition/blob/main/kan_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HipfT3cIs5ny"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import directed_hausdorff\n",
        "from torch.utils.data import DataLoader, TensorDataset\n"
      ],
      "execution_count": 1,
      "outputs": [],
      "id": "HipfT3cIs5ny"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oueJQ0uRs5nz"
      },
      "source": [
        "# ─── Spline Basis ─────────────────────────────────────────────\n",
        "class SplineBasis(nn.Module):\n",
        "    def __init__(self, num_basis=16, domain=(-1.0, 1.0)):\n",
        "        super().__init__()\n",
        "        self.num_basis = num_basis\n",
        "        self.domain = domain\n",
        "        self.register_buffer('knots', torch.linspace(domain[0], domain[1], num_basis))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(-1)\n",
        "        distances = torch.abs(x - self.knots)\n",
        "        basis = torch.clamp(1 - distances * self.num_basis, min=0)\n",
        "        return basis\n"
      ],
      "execution_count": 2,
      "outputs": [],
      "id": "oueJQ0uRs5nz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5cXC-jfs5nz"
      },
      "source": [
        "# ─── KAN Layer 1D ─────────────────────────────────────────────\n",
        "class KANLayer1D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_basis=16):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_basis = num_basis\n",
        "        self.spline = SplineBasis(num_basis)\n",
        "        self.coeffs = nn.Parameter(torch.randn(out_channels, in_channels, num_basis))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, in_channels)\n",
        "        basis_vals = self.spline(x)  # shape: (batch_size, in_channels, num_basis)\n",
        "        out = torch.einsum('bic,oic->bo', basis_vals, self.coeffs)\n",
        "        return out\n"
      ],
      "execution_count": 9,
      "outputs": [],
      "id": "T5cXC-jfs5nz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g0RVxv1s5n0"
      },
      "source": [
        "# ─── KAN-Enhanced SE Block ───────────────────────────────────\n",
        "class KANSEBlock(nn.Module):\n",
        "    def __init__(self, in_channels, reduction=16, num_basis=16):\n",
        "        super().__init__()\n",
        "        mid_channels = max(1, in_channels // reduction)\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.kan1 = KANLayer1D(in_channels, mid_channels, num_basis)\n",
        "        self.kan2 = KANLayer1D(mid_channels, in_channels, num_basis)\n",
        "        self.activation = nn.ReLU(inplace=True)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        z = self.global_pool(x).view(B, C)\n",
        "        h = self.activation(self.kan1(z))\n",
        "        s = self.sigmoid(self.kan2(h)).view(B, C, 1, 1)\n",
        "        return x * s\n"
      ],
      "execution_count": 10,
      "outputs": [],
      "id": "2g0RVxv1s5n0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b8jFiw7s5n0"
      },
      "source": [
        "# ─── Evaluation Metrics ──────────────────────────────────────\n",
        "def dice_coefficient(pred, target, epsilon=1e-6):\n",
        "    pred = (pred > 0.5).float()\n",
        "    target = (target > 0.5).float()\n",
        "    intersection = (pred * target).sum(dim=(2, 3))\n",
        "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
        "    dice = (2. * intersection + epsilon) / (union + epsilon)\n",
        "    return dice.mean().item()\n",
        "\n",
        "def hd95(pred, target):\n",
        "    pred_np = pred.squeeze().detach().cpu().numpy()\n",
        "    target_np = target.squeeze().detach().cpu().numpy()\n",
        "    pred_coords = np.argwhere(pred_np > 0)\n",
        "    target_coords = np.argwhere(target_np > 0)\n",
        "    if len(pred_coords) == 0 or len(target_coords) == 0:\n",
        "        return np.inf\n",
        "    hd_forward = directed_hausdorff(pred_coords, target_coords)[0]\n",
        "    hd_backward = directed_hausdorff(target_coords, pred_coords)[0]\n",
        "    return np.percentile([hd_forward, hd_backward], 95)\n"
      ],
      "execution_count": 11,
      "outputs": [],
      "id": "0b8jFiw7s5n0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ldr3Jobs5n0"
      },
      "source": [
        "# ─── Training Loop ────────────────────────────────────────────\n",
        "def train_model(model, dataloader, optimizer, criterion, num_epochs=5, device='cpu'):\n",
        "    model.to(device)\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for images, masks in dataloader:\n",
        "            images, masks = images.to(device), masks.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n",
        "    return model\n"
      ],
      "execution_count": 12,
      "outputs": [],
      "id": "6ldr3Jobs5n0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0g-2pCks5n0"
      },
      "source": [
        "# ─── Example CNN Using KANSEBlock ─────────────────────────────\n",
        "class SampleKANUNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            KANSEBlock(out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n"
      ],
      "execution_count": 13,
      "outputs": [],
      "id": "c0g-2pCks5n0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3zKwvsMs5n0",
        "outputId": "8ddda8b9-4dff-41fd-fb19-17bbbfe45368"
      },
      "source": [
        "# ─── Synthetic Test ───────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    # Synthetic data (B, C, H, W)\n",
        "    images = torch.rand(10, 1, 64, 64)\n",
        "    masks = (images > 0.5).float()\n",
        "\n",
        "    dataset = TensorDataset(images, masks)\n",
        "    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "    # Model setup\n",
        "    model = SampleKANUNetBlock(in_channels=1, out_channels=1)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    # Train\n",
        "    trained_model = train_model(model, dataloader, optimizer, criterion, num_epochs=100)\n",
        "\n",
        "    # Evaluate\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, masks in dataloader:\n",
        "            outputs = torch.sigmoid(model(images))\n",
        "            dice = dice_coefficient(outputs, masks)\n",
        "            hd = hd95(outputs, masks)\n",
        "            print(f\"Dice: {dice:.4f} | HD95: {hd:.2f}\")\n",
        "            break  # evaluate one batch\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 3.5367\n",
            "Epoch [2/100], Loss: 3.5304\n",
            "Epoch [3/100], Loss: 3.5248\n",
            "Epoch [4/100], Loss: 3.5181\n",
            "Epoch [5/100], Loss: 3.5111\n",
            "Epoch [6/100], Loss: 3.5038\n",
            "Epoch [7/100], Loss: 3.4964\n",
            "Epoch [8/100], Loss: 3.4883\n",
            "Epoch [9/100], Loss: 3.4800\n",
            "Epoch [10/100], Loss: 3.4712\n",
            "Epoch [11/100], Loss: 3.4617\n",
            "Epoch [12/100], Loss: 3.4514\n",
            "Epoch [13/100], Loss: 3.4402\n",
            "Epoch [14/100], Loss: 3.4281\n",
            "Epoch [15/100], Loss: 3.4151\n",
            "Epoch [16/100], Loss: 3.4008\n",
            "Epoch [17/100], Loss: 3.3857\n",
            "Epoch [18/100], Loss: 3.3687\n",
            "Epoch [19/100], Loss: 3.3510\n",
            "Epoch [20/100], Loss: 3.3311\n",
            "Epoch [21/100], Loss: 3.3095\n",
            "Epoch [22/100], Loss: 3.2874\n",
            "Epoch [23/100], Loss: 3.2628\n",
            "Epoch [24/100], Loss: 3.2369\n",
            "Epoch [25/100], Loss: 3.2154\n",
            "Epoch [26/100], Loss: 3.1900\n",
            "Epoch [27/100], Loss: 3.1665\n",
            "Epoch [28/100], Loss: 3.1440\n",
            "Epoch [29/100], Loss: 3.1218\n",
            "Epoch [30/100], Loss: 3.1007\n",
            "Epoch [31/100], Loss: 3.0804\n",
            "Epoch [32/100], Loss: 3.0620\n",
            "Epoch [33/100], Loss: 3.0451\n",
            "Epoch [34/100], Loss: 3.0301\n",
            "Epoch [35/100], Loss: 3.0166\n",
            "Epoch [36/100], Loss: 3.0048\n",
            "Epoch [37/100], Loss: 2.9942\n",
            "Epoch [38/100], Loss: 2.9847\n",
            "Epoch [39/100], Loss: 2.9761\n",
            "Epoch [40/100], Loss: 2.9682\n",
            "Epoch [41/100], Loss: 2.9610\n",
            "Epoch [42/100], Loss: 2.9542\n",
            "Epoch [43/100], Loss: 2.9477\n",
            "Epoch [44/100], Loss: 2.9417\n",
            "Epoch [45/100], Loss: 2.9359\n",
            "Epoch [46/100], Loss: 2.9303\n",
            "Epoch [47/100], Loss: 2.9250\n",
            "Epoch [48/100], Loss: 2.9200\n",
            "Epoch [49/100], Loss: 2.9152\n",
            "Epoch [50/100], Loss: 2.9105\n",
            "Epoch [51/100], Loss: 2.9060\n",
            "Epoch [52/100], Loss: 2.9015\n",
            "Epoch [53/100], Loss: 2.8971\n",
            "Epoch [54/100], Loss: 2.8928\n",
            "Epoch [55/100], Loss: 2.8885\n",
            "Epoch [56/100], Loss: 2.8843\n",
            "Epoch [57/100], Loss: 2.8801\n",
            "Epoch [58/100], Loss: 2.8760\n",
            "Epoch [59/100], Loss: 2.8720\n",
            "Epoch [60/100], Loss: 2.8679\n",
            "Epoch [61/100], Loss: 2.8639\n",
            "Epoch [62/100], Loss: 2.8600\n",
            "Epoch [63/100], Loss: 2.8561\n",
            "Epoch [64/100], Loss: 2.8523\n",
            "Epoch [65/100], Loss: 2.8485\n",
            "Epoch [66/100], Loss: 2.8447\n",
            "Epoch [67/100], Loss: 2.8410\n",
            "Epoch [68/100], Loss: 2.8373\n",
            "Epoch [69/100], Loss: 2.8336\n",
            "Epoch [70/100], Loss: 2.8300\n",
            "Epoch [71/100], Loss: 2.8264\n",
            "Epoch [72/100], Loss: 2.8228\n",
            "Epoch [73/100], Loss: 2.8193\n",
            "Epoch [74/100], Loss: 2.8158\n",
            "Epoch [75/100], Loss: 2.8051\n",
            "Epoch [76/100], Loss: 2.8150\n",
            "Epoch [77/100], Loss: 2.8154\n",
            "Epoch [78/100], Loss: 2.8134\n",
            "Epoch [79/100], Loss: 2.8123\n",
            "Epoch [80/100], Loss: 2.8105\n",
            "Epoch [81/100], Loss: 2.8085\n",
            "Epoch [82/100], Loss: 2.7890\n",
            "Epoch [83/100], Loss: 2.7722\n",
            "Epoch [84/100], Loss: 2.8280\n",
            "Epoch [85/100], Loss: 2.8334\n",
            "Epoch [86/100], Loss: 2.8274\n",
            "Epoch [87/100], Loss: 2.8195\n",
            "Epoch [88/100], Loss: 2.8154\n",
            "Epoch [89/100], Loss: 2.8141\n",
            "Epoch [90/100], Loss: 2.8049\n",
            "Epoch [91/100], Loss: 2.8116\n",
            "Epoch [92/100], Loss: 2.8102\n",
            "Epoch [93/100], Loss: 2.7997\n",
            "Epoch [94/100], Loss: 2.8016\n",
            "Epoch [95/100], Loss: 2.7620\n",
            "Epoch [96/100], Loss: 2.7623\n",
            "Epoch [97/100], Loss: 2.7512\n",
            "Epoch [98/100], Loss: 2.7635\n",
            "Epoch [99/100], Loss: 2.7382\n",
            "Epoch [100/100], Loss: 2.7704\n",
            "Dice: 0.8940 | HD95: 1.34\n"
          ]
        }
      ],
      "id": "G3zKwvsMs5n0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}